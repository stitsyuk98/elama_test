# elama_test

Тестовое задание на вакансию data engineer

Требуется:
Поднять на локальной машине Airflow
Поднять на локальной машине PostgreSQL
Создать проект в Google BigQuery
Создать в Airflow даг, который:
1) Заливает прилагаемые csv-файлы в соответствующие таблицы в PostgreSQL
2) Создает в PostgreSQL материализованное представление, решающее следующую задачу:
В таблице webinar.csv находится список посетителей вебинара, который прошел 1 апреля 2016 г.
Выбрать тех посетителей, кто впервые зарегистрировался в еЛаме (users.csv) после вебинара,
и для каждого из них посчитать сумму его пополнений в системе (transactions.csv).
Под посетителем вебинара понимается один email.
3) (Опционально) Отправляет полученную таблицу в Google BigQuery (используя соответствующий API)
Если отправить таблицу в BigQuery через даг не получилось, загрузить её туда вручную через интерфейс BigQuery.
 



В качестве результата предоставить:
* Код дага и его тасков в gitHub + лог исполнения
* текст SQL-запроса материализованного представления + скриншот с его содержимым
* скриншот таблицы в BigQuery

_____________________________________________________________________________________


cd airflow/
docker-compose up
docker exec -it airflow_webserver_1 bash
pip install --upgrade google-auth
pip install --upgrade google-cloud-bigquery
pip install pyarrow

- В папке dags  создать папку keys  c  кредами к bigquery
- Создать коннектор Postgres в интерфейсе airflow
- В файле worker прописать свою переменную TABLE_ID

cd postgres/
docker-compose -f stack.yml up